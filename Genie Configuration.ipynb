{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05a5aaa-8d9d-42f1-91c1-e9bee830c49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the dataset\n",
    "Use the dataset `Formula 1 datasets` which contains data from 1950 to 2022.\n",
    "This dataset is available on Kaggle and other website called Ergast\n",
    "\n",
    "https://ergast.com/mrd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a48d5b-4ce3-40c6-9fd9-fa1c9617d1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the Catalog and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5058e500-a8a5-4aa6-a259-003d12ec5909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"douglas_s_m0pf_da\"\n",
    "db_name = \"genie_teams_integration\"\n",
    "volume = \"f1_dataset_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5d1236-762a-476f-97ea-674f8b580845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a catalog called _demos and a database called genie_data in it\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog};\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog}.{db_name};\")\n",
    "\n",
    "\n",
    "#Create a volume in the database genie_data\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{db_name}.{volume};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cc7ec3-08a6-4bdf-a6f7-8a689e1e6b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CATALOG'] = catalog\n",
    "os.environ['DB_NAME'] = db_name\n",
    "os.environ['VOLUME'] = volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b168cf4-fc7a-4ab5-aad3-399f802fdfcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "# Define the download URL and target directory\n",
    "URL=\"https://raw.githubusercontent.com/rubenv/ergast-mrd/master/f1db_csv.zip\"\n",
    "DEST_DIR=\"/Volumes/${CATALOG}/${DB_NAME}/${VOLUME}\"\n",
    "\n",
    "\n",
    "# Download the ZIP file\n",
    "wget -O $DEST_DIR/f1db_csv.zip $URL\n",
    "\n",
    "# Unzip the file\n",
    "unzip $DEST_DIR/f1db_csv.zip -d $DEST_DIR\n",
    "\n",
    "# Clean up the ZIP file\n",
    "rm $DEST_DIR/f1db_csv.zip\n",
    "\n",
    "# List the contents of the directory\n",
    "echo \"Files extracted to $DEST_DIR:\"\n",
    "ls $DEST_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f2ef32-a5a5-4b23-b015-97950d8a0b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a delta table from the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17336c2-6bdf-428b-add5-fb572bf49e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Folder containing the CSV files\n",
    "input_folder = f\"/Volumes/{catalog}/{db_name}/{volume}\"\n",
    "\n",
    "# Ensure the database exists in the specified catalog\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog}.{db_name}\")\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Full path to the CSV file\n",
    "        file_path = f\"{input_folder}/{file_name}\"\n",
    "        \n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "                       .option(\"header\", \"true\") \\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .load(file_path)\n",
    "        \n",
    "        # Create a table name based on the file name (remove the .csv extension)\n",
    "        table_name = file_name.replace(\".csv\", \"\")\n",
    "        \n",
    "        # Fully qualified table name\n",
    "        fully_qualified_table_name = f\"{catalog}.{db_name}.{table_name}\"\n",
    "        \n",
    "        # Register the DataFrame as a table in the catalog\n",
    "        df.write.format(\"delta\").saveAsTable(fully_qualified_table_name)\n",
    "        \n",
    "        print(f\"Table '{fully_qualified_table_name}' created from file '{file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e0d489-b860-413b-b4f1-154479e151af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all tables in the specified catalog and database\n",
    "tables_df = spark.sql(\n",
    "    f\"SHOW TABLES IN {catalog}.{db_name}\"\n",
    ").select(\n",
    "    \"tableName\"\n",
    ")\n",
    "tables = [\n",
    "    row['tableName'] for row in tables_df.collect()\n",
    "]\n",
    "\n",
    "# Generate DESC for each table\n",
    "for table in tables:\n",
    "    fully_qualified_table_name = f\"{catalog}.{db_name}.{table}\"\n",
    "    print(f\"DESC for table '{fully_qualified_table_name}':\")\n",
    "    spark.sql(f\"DESC {fully_qualified_table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dff9b21-0aed-4fa6-a409-dc547837954b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the MV`s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855bc942-a1bc-43ac-aaa3-2122989fa600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the pyspark functions\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "# ==============================================================================\n",
    "# Defining Bronze and Silver Tables\n",
    "# ==============================================================================\n",
    "\n",
    "# Source tables (Bronze)\n",
    "circuits_table = f\"{CATALOG}.{SCHEMA}.circuits\"\n",
    "constructors_table = f\"{CATALOG}.{SCHEMA}.constructors\"\n",
    "drivers_table = f\"{CATALOG}.{SCHEMA}.drivers\"\n",
    "races_table = f\"{CATALOG}.{SCHEMA}.races\"\n",
    "results_table = f\"{CATALOG}.{SCHEMA}.results\"\n",
    "sprint_results_table = f\"{CATALOG}.{SCHEMA}.sprint_results\"\n",
    "status_table = f\"{CATALOG}.{SCHEMA}.status\"\n",
    "\n",
    "# Target Tables (Silver)\n",
    "silver_table_name = f\"{CATALOG}.{SCHEMA}.fct_race_results\"\n",
    "\n",
    "# ==============================================================================\n",
    "# Load bronze tables as dataframes\n",
    "# ==============================================================================\n",
    "print(\"Loading bronze tables...\")\n",
    "circuits_df = spark.table(circuits_table)\n",
    "constructors_df = spark.table(constructors_table)\n",
    "drivers_df = spark.table(drivers_table)\n",
    "races_df = spark.table(races_table)\n",
    "results_df = spark.table(results_table)\n",
    "sprint_results_df = spark.table(sprint_results_table)\n",
    "status_df = spark.table(status_table)\n",
    "\n",
    "# ==============================================================================\n",
    "# Combining Grand Prix and Sprint\n",
    "# ==============================================================================\n",
    "print(\"Combining Grand Prix and Sprint...\")\n",
    "\n",
    "# Add 'race_type' and prepare to join\n",
    "results_unioned_df = results_df.withColumn(\"race_type\", lit(\"Grand Prix\"))\n",
    "\n",
    "# Add coluns to sprint_results\n",
    "sprint_results_unioned_df = sprint_results_df \\\n",
    "    .withColumn(\"race_type\", lit(\"Sprint Race\")) \\\n",
    "    .withColumn(\"rank\", lit(None).cast(\"string\")) \\\n",
    "    .withColumn(\"fastestLapSpeed\", lit(None).cast(\"string\"))\n",
    "\n",
    "combined_results_df = results_unioned_df.unionByName(sprint_results_unioned_df)\n",
    "\n",
    "# ==============================================================================\n",
    "# Rename coluns for a better column identification\n",
    "# ==============================================================================\n",
    "races_renamed_df = races_df.withColumnRenamed(\"name\", \"race_name\")\n",
    "circuits_renamed_df = circuits_df.withColumnRenamed(\"name\", \"circuit_name\")\n",
    "constructors_renamed_df = constructors_df.withColumnRenamed(\"name\", \"constructor_name\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Join the table with the dimentions\n",
    "# ==============================================================================\n",
    "final_df = (\n",
    "    combined_results_df\n",
    "    .join(races_renamed_df, \"raceId\", \"inner\")\n",
    "    .join(drivers_df, \"driverId\", \"inner\")\n",
    "    .join(constructors_renamed_df, \"constructorId\", \"inner\")\n",
    "    .join(circuits_renamed_df, \"circuitId\", \"inner\")\n",
    "    .join(status_df, \"statusId\", \"inner\")\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Selecionar e Renomear as colunas finais\n",
    "#    (Equivalente ao SELECT principal da query SQL)\n",
    "# ==============================================================================\n",
    "print(\"Selecionando e formatando colunas finais...\")\n",
    "silver_df = final_df.select(\n",
    "    # Chaves e IDs\n",
    "    col(\"resultId\"),\n",
    "    col(\"raceId\"),\n",
    "    col(\"driverId\"),\n",
    "    col(\"constructorId\"),\n",
    "    col(\"circuitId\"),\n",
    "    col(\"statusId\"),\n",
    "    \n",
    "    # Informações da Corrida e Circuito\n",
    "    col(\"year\").alias(\"race_year\"),\n",
    "    col(\"round\"),\n",
    "    col(\"race_name\"),\n",
    "    col(\"date\").alias(\"race_date\"),\n",
    "    col(\"circuit_name\"),\n",
    "    col(\"location\").alias(\"circuit_location\"),\n",
    "    col(\"country\").alias(\"circuit_country\"),\n",
    "    col(\"race_type\"),\n",
    "    \n",
    "    # Informações do Piloto\n",
    "    concat(col(\"forename\"), lit(\" \"), col(\"surname\")).alias(\"driver_name\"),\n",
    "    col(\"drivers.number\").alias(\"driver_number\"), # Qualificar para evitar ambiguidade\n",
    "    col(\"drivers.nationality\").alias(\"driver_nationality\"),\n",
    "    col(\"dob\").alias(\"driver_date_of_birth\"),\n",
    "    \n",
    "    # Informações do Construtor\n",
    "    col(\"constructor_name\"),\n",
    "    col(\"constructors.nationality\").alias(\"constructor_nationality\"),\n",
    "    \n",
    "    # Métricas e Resultados da Corrida\n",
    "    col(\"grid\").alias(\"starting_grid\"),\n",
    "    col(\"positionOrder\").alias(\"finishing_position\"),\n",
    "    col(\"points\"),\n",
    "    col(\"laps\"),\n",
    "    col(\"results.time\").alias(\"total_race_time\"), # Qualificar para evitar ambiguidade\n",
    "    col(\"fastestLapTime\"),\n",
    "    col(\"fastestLapSpeed\"),\n",
    "    col(\"status\").alias(\"finishing_status\")\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. Salvar o DataFrame resultante como uma tabela Delta (camada Silver)\n",
    "# ==============================================================================\n",
    "print(f\"Salvando tabela silver em: {silver_table_name}\")\n",
    "(\n",
    "    silver_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\") # Boa prática durante o desenvolvimento\n",
    "    .saveAsTable(silver_table_name)\n",
    ")\n",
    "\n",
    "print(\"Tabela silver criada com sucesso! ✅\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. (Opcional) Exibir uma amostra dos dados da nova tabela\n",
    "# ==============================================================================\n",
    "display(spark.table(silver_table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd834126-720d-4b1d-a66e-73730f14a924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {catalog}.{db_name}.vw_race_results_enriched\n",
    "    COMMENT 'View that consolidates Grand Prix and Sprint results with details on drivers, constructors, and circuits.'\n",
    "    AS\n",
    "    WITH combined_results AS (\n",
    "      SELECT\n",
    "        resultId, raceId, driverId, constructorId, number, grid, position,\n",
    "        positionText, positionOrder, points, laps, time, milliseconds,\n",
    "        fastestLap, rank, fastestLapTime, fastestLapSpeed, statusId,\n",
    "        'Grand Prix' AS race_type\n",
    "      FROM\n",
    "        {catalog}.{db_name}.results\n",
    "      UNION ALL\n",
    "      SELECT\n",
    "        resultId, raceId, driverId, constructorId, number, grid, position,\n",
    "        positionText, positionOrder, points, laps, time, milliseconds,\n",
    "        fastestLap,\n",
    "        CAST(NULL AS STRING) as rank,\n",
    "        fastestLapTime,\n",
    "        CAST(NULL AS STRING) as fastestLapSpeed,\n",
    "        statusId,\n",
    "        'Sprint Race' AS race_type\n",
    "      FROM\n",
    "        {catalog}.{db_name}.sprint_results\n",
    "    )\n",
    "    SELECT\n",
    "      res.resultId, res.raceId, res.driverId, res.constructorId, r.circuitId, res.statusId,\n",
    "      r.year AS race_year, r.round, r.name AS race_name, r.date AS race_date,\n",
    "      circ.name AS circuit_name, circ.location AS circuit_location, circ.country AS circuit_country,\n",
    "      res.race_type,\n",
    "      CONCAT(d.forename, ' ', d.surname) AS driver_name,\n",
    "      d.number AS driver_number, d.nationality AS driver_nationality, d.dob AS driver_date_of_birth,\n",
    "      c.name AS constructor_name, c.nationality AS constructor_nationality,\n",
    "      res.grid AS starting_grid, res.positionOrder AS finishing_position, res.points, res.laps,\n",
    "      res.time AS total_race_time, res.fastestLapTime, res.fastestLapSpeed,\n",
    "      st.status AS finishing_status\n",
    "    FROM\n",
    "      combined_results res\n",
    "      JOIN {catalog}.{db_name}.races r ON res.raceId = r.raceId\n",
    "      JOIN {catalog}.{db_name}.drivers d ON res.driverId = d.driverId\n",
    "      JOIN {catalog}.{db_name}.constructors c ON res.constructorId = c.constructorId\n",
    "      JOIN {catalog}.{db_name}.circuits circ ON r.circuitId = circ.circuitId\n",
    "      JOIN {catalog}.{db_name}.status st ON res.statusId = st.statusId;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8275531a-1793-42d2-98bc-c86f4e2c3d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM {catalog}.{db_name}.vw_race_results_enriched LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788dfb04-c994-47e3-87b6-62274be9b4b7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756080453442}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT DISTINCT race_year FROM {catalog}.{db_name}.vw_race_results_enriched\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f4b1b2-b466-43f6-b0d5-9b3cad6931cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5261002272439757,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Genie Configuration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
